---
title: "Report"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Ariel Camacho"
date: "11/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Data retrieval

```{r caret}
library(caret)
library(ggplot2)
```

We load the training data:
```{r data1}
dataDLA <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
```

Now we explore the data:

```{r data2}
dim(dataDLA)
```

We have 19622 instances with 160 attributes. We want to predict "classe". Let's see how many levels there are:
```{r data33}
levels(dataDLA$classe)
```

```{r data22}
head(dataDLA)
```

We observe NA values. Let's found out the number of NAs in each column:

```{r data3}
colSums(is.na(dataDLA))
```

```{r dataempty}
colSums(dataDLA == "")
```

Observe that from the 19622 instances, we have 19216 instances with NA atttributes such as max_roll_belt. Also, there are empty cells. We drop them out:

```{r dataNA}
dataDLA2 <- dataDLA[colSums(!is.na(dataDLA)) == 19622]
dataDLA2 <- dataDLA2[, colSums(dataDLA2 != "") == 19622 ]
print(head(dataDLA2))
print(dim(dataDLA2))
```

We now have 60 attributes. Observe that the first 6 columns are attributes that are not informative for predictive purposes.

```{r dataV3}
training <- dataDLA2[, 8:60]
```

## Data exploration

Let's explore some features to see if we can distinguish relevant attributes.

```{r plot1}
qplot(roll_belt,pitch_belt,data=training,colour=classe)
```

```{r plot2}
qplot(gyros_belt_x,gyros_belt_y,data=training,colour=classe)
```

```{r plot3}
qplot(accel_belt_x,accel_belt_y,data=training,colour=classe)
```

```{r plot11}
qplot(roll_dumbbell,pitch_dumbbell,data=training,colour=classe)
```

Plots show that classe "E" are predominant, and that in general the classes are not easily separated for the chosed attributes. We may not expect high accuracy in predicting the class. 

## Fitting models


```{r control}
set.seed(2020)
fitControl <- trainControl(method = "cv", number = 10)
model_lda <- train(classe ~ ., data = training, 
                 method = "lda", 
                 trControl = fitControl)
model_lda
```

```{r control2}
model_rf <- train(classe ~ ., data = training, 
                 method = "rf", 
                 trControl = fitControl)
model_rf
```

We get 70.24% mean accuracy with LDA and 99.36% mean accuracy with RF with 10-fold cross-validation. We may expect over-fitting with RF.


## Testing models

We load the  testing data:
```{r datatest}
library(dplyr)
dataDLA_test <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))

columns <- colnames(training)
columns <- columns[-53]
testing <- select(dataDLA_test, columns)
```

```{r predict}
pred_lda <- predict(model_lda, testing)
pred_lda
```

```{r predict2}
pred_rf <- predict(model_rf, testing)
pred_rf
```

```{r predict3}
postResample(pred_lda, pred_rf)
```

(Data retrieved from: <http://groupware.les.inf.puc-rio.br/har#dataset>.)